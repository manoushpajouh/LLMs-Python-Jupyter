# -*- coding: utf-8 -*-
"""Named Entity Recognition Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nLjnyFmShgSoTRKo8MmVz_JyZD2QAWu7

# Program 6: Named Entity Recognition

Below is an implemented named entity recognition (NER) system. The named
entity recognition task consists of finding entities in a text by determining
whether each word is or isn't a named entity, which is in essence a sequence
labeling task.

This model uses the
[CoNLL 2003 dataset](https://paperswithcode.com/dataset/conll-2003).
It contains sentences extracted from the Reuters corpus and annotated in the IOB
format. The IOB format tags each token (word) as being [I]nside, [O]utside, or
[B]eginning of a named entity. The O tag is used for words that are not entites,
while the B is used for the first word of an entity and I for every other word
in that same entity. This means that I can extract multiword entities as well
with clearly separated boundaries.

For example, the sentence

> The European Commission said on Thursday it disagreed with German advice [...]

would be tagged as

> O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O, [...]

Note that the IOB format lets us know that "European Commission" is one entity instead of two separate ones.

### Pretraining and fine-tuning

Instead of training a model from scratch, this model uses the
pretraining-and-finetuning approach. This approach enables me to save on
resources while achieving good results.

I am using libraries developed by [HuggingFace](https://huggingface.co/).

# Objectives

1. Familiarize myself with the `transformers` library for training
and using Transformer models
2. Fine-tune a pretrained model for NER
3. Compare different pretrained models

# Implementation
"""

# install required libraries
! pip install torch transformers[torch] datasets

# load the dataset that we're going to use
from datasets import load_dataset
dataset = load_dataset("conll2003", trust_remote_code=True)

# inspect the dataset
example = dataset["train"][0]
print(example)

"""Below, I am importing the two most important classes in this program: the
model class and the tokenizer class. In the `transformers` library, each model
architecture has a specific class and a specific tokenizer, since the individual
architectures may use different approaches to tokenization (e.g. the total number
of tokens).

In the first part, I am going to use the
[BERT](https://arxiv.org/abs/1810.04805) pretrained model, one of the most
popular encoder-only architectures. Specifically, I want to use a BERT instance
adapted for sequence labeling (also known as token classification).
"""

from transformers import BertTokenizerFast, BertForTokenClassification # import the appropriate model class

"""Next, I am going to load pretrained weights into the tokenizer and model.
This is easily done by calling the `.from_pretrained()` method that downloads
the data from HuggingFace. The parameter to that method is the name of the
specific model (not model architecture) you want to use.

Here, we want to use the `bert-base-uncased` model.

The `from_pretrained()` method on the model additionally takes a `num_labels`
keyword argument. This is the number of possible labels (tags) for each word.
"""

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels = 9) # call .from_pretrained on the model class you imported

"""Now, I am going to define a method that processes a dataset row
(see the example above) into tensors.

Specifically, I will call the tokenizer with the correct parameters.
Additionally, return the NER tags of each example as the labels (targets).
"""

# utility function to align labels to tokens after tokenization -- don't change this
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Special token
            new_labels.append(-100)
        else:
            # Same word as previous token
            label = labels[word_id]
            # If the label is B-XXX we change it to I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels


def tokenize_and_preprocess(example):
    # fill in the parameters
    tokens = tokenizer(example['tokens'],
                       truncation=True,
                       padding='max_length',
                       max_length=128,
                       return_tensors="pt",
                       is_split_into_words = True,)
    # NER tags (dataset already annotated just extract them)
    labels = example['ner_tags']
    # align the labels
    new_labels = align_labels_with_tokens(labels, tokens.word_ids())
    # pad the labels to match the length of the input
    new_labels = new_labels + [0 for _ in range(128 - len(new_labels))]
    return {'input_ids': tokens['input_ids'].squeeze(), 'attention_mask':
            tokens['attention_mask'].squeeze(), 'labels': new_labels}

# apply the function on your data
train_dataset = dataset["train"].map(tokenize_and_preprocess)
eval_dataset = dataset["validation"].map(tokenize_and_preprocess)

"""Using `Trainer` class from the `transformers` library to
fine-tune the model. Note that this is different from PyTorch Lightning `Traniner`.

"""

from transformers import Trainer, TrainingArguments

# Define the training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs = 1,
    learning_rate= 0.00003,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    report_to="none"
)

"""Below is the code needed for the evaluation of the model."""

! pip install seqeval evaluate

import evaluate
import numpy as np


metric = evaluate.load("seqeval")
label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }

"""Next, define the `Trainer`."""

trainer = Trainer(
    model=model,                            # the instantiated Transformers model to be trained
    args=training_args,                     # training arguments, defined above
    train_dataset=train_dataset,            # the training dataset
    eval_dataset=eval_dataset,              # the evaluation dataset
    compute_metrics=compute_metrics,        # defines how to evaluate the model
)

"""Now, train the model by calling the `train()` method on the trainer."""

trainer.train()

"""Now, evaluate the data by calling the `evaluate` method on the trainer.:"""

# evaluate the model
evaluation_results = trainer.evaluate()
print(evaluation_results)

"""## Different pretrained models

Now, repeat the training process with two different pretrained models.
"""

! pip install torch transformers[torch] datasets
! pip install seqeval

from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline
from transformers import Trainer, TrainingArguments


tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

nlp = pipeline("ner", model=model, tokenizer=tokenizer)
# utility function to align labels to tokens after tokenization -- don't change this
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Special token
            new_labels.append(-100)
        else:
            # Same word as previous token
            label = labels[word_id]
            # If the label is B-XXX we change it to I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels


def tokenize_and_preprocess(example):
    tokens = tokenizer(example['tokens'],
                       truncation=True,
                       padding='max_length',
                       max_length=128,
                       return_tensors="pt",
                       is_split_into_words = True,) #  fill in the parameters
    labels = example['ner_tags'] # NER tags (dataset already annotated just extract them)
    # align the labels
    new_labels = align_labels_with_tokens(labels, tokens.word_ids())
    # pad the labels to match the length of the input
    new_labels = new_labels + [0 for _ in range(128 - len(new_labels))]
    return {'input_ids': tokens['input_ids'].squeeze(), 'attention_mask': 
            tokens['attention_mask'].squeeze(), 'labels': new_labels}

# apply the function on your data
train_dataset = dataset["train"].map(tokenize_and_preprocess)
eval_dataset = dataset["validation"].map(tokenize_and_preprocess)


import evaluate
import numpy as np


metric = evaluate.load("seqeval")
label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # Remove ignored index (special tokens) and convert to labels
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }

training_args = TrainingArguments(
    output_dir='./results',           # output directory
    num_train_epochs = 1,
    learning_rate= 0.00003,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    report_to="none"
)

trainer2 = Trainer(
    model=model,                            # the instantiated Transformers model to be trained
    args=training_args,                     # training arguments, defined above
    train_dataset=train_dataset,            # the training dataset
    eval_dataset=eval_dataset,              # the evaluation dataset
    compute_metrics=compute_metrics,        # defines how to evaluate the model
)

trainer2.train()

# evaluate the model
evaluation_results = trainer2.evaluate()
print(evaluation_results)